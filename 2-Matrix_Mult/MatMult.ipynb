{"nbformat_minor": 2, "cells": [{"execution_count": null, "cell_type": "code", "source": "import org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 1, "cell_type": "code", "source": "//Realiza a multiplica\u00e7\u00e3o de matrizes serial \ndef matMultSerial(A: Array[Array[Float]], B: Array[Array[Float]]): Array[Array[Float]] = {\n    val row_count = A.length\n    val column_count = B(0).length\n    val R = Array.ofDim[Float](row_count, column_count)\n\n    for ( i <- 0 to (row_count - 1)) {\n        for (j <- 0 to (column_count - 1)) {\n            var sum = 0.0f\n            for (k <- 0 to (B.length - 1)) {\n                sum += (A(i)(k) * B(k)(j))\n            }\n            R(i)(j) = sum\n        }\n    }\n\n    return R\n}\n\n//Realiza a multiplicacao de matrizes distribuida com Spark\ndef coordinateMatrixMultiply(A: CoordinateMatrix, B: CoordinateMatrix): CoordinateMatrix = {\n    val A_map = A.entries.map({ case MatrixEntry(i, j, v) => (j, (i, v)) })\n    val B_map = B.entries.map({ case MatrixEntry(j, k, w) => (j, (k, w)) })\n\n    val productEntries = A_map\n        .join(B_map)\n        .map({ case (_, ((i, v), (k, w))) => ((i, k), (v * w)) })\n        .reduceByKey(_ + _)\n        .map({ case ((i, k), sum) => MatrixEntry(i, k, sum) })\n\n    return new CoordinateMatrix(productEntries)\n}\n\n\n//Converte uma matriz para CoordinateMatrix para executar no coordinateMatrixMultiply\ndef convertArrayToCoordMat(Mat: Array[Array[Float]]): CoordinateMatrix = {\n    var array = new Array[MatrixEntry]( (Mat.length * Mat(0).length) )\n    var index = 0\n    for ( i <- 0 to (Mat.length - 1)) {\n        for (j <- 0 to (Mat(i).length - 1)) {\n            array(index) = new MatrixEntry(i, j, Mat(i)(j))\n            index += 1\n        }\n    }\n    val entries_A = sc.parallelize(array)\n    //entries_A.collect().foreach(println /*row => println(row.mkString(\" \"))*/)\n    return new CoordinateMatrix(entries_A)\n}\n\n//Criar matrizes Random\nval SIZE = 100\nval A = Array.fill(SIZE, SIZE)(scala.util.Random.nextFloat())\nval B = Array.fill(SIZE, SIZE)(scala.util.Random.nextFloat())\n\n//Converte para CoordinateMatrix\nval coordMatA = convertArrayToCoordMat(A)\nval coordMatB = convertArrayToCoordMat(B) \n\n//Executa serial\n/*val t = System.nanoTime\nval C = matMultSerial(A, B)\nprintln( (System.nanoTime - t) / 1e9d ) */\n\n//Executa ver\u00e3o paralela com spark\nval t = System.nanoTime\nval C = coordinateMatrixMultiply(coordMatA, coordMatB)\nprintln( (System.nanoTime - t) / 1e9d )", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>12</td><td>application_1530241154787_0018</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-matrix.qsrifx0t4o4uti14hs5o2jyn1f.hx.internal.cloudapp.net:8088/proxy/application_1530241154787_0018/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn1-matrix.qsrifx0t4o4uti14hs5o2jyn1f.hx.internal.cloudapp.net:30060/node/containerlogs/container_1530241154787_0018_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n0.409747878"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}